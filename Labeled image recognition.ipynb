{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dillon/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# All imports for ease here\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, os.path\n",
    "\n",
    "# init local path constants\n",
    "raid_dir = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/'\n",
    "raid_train_dir = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/train/'\n",
    "raid_valid_dir = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/valid/'\n",
    "raid_test_dir = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRetrieval/test/'\n",
    "train_csv = '~/Documents/Kaggle/GoogleLandmarkRecognition/train.csv'\n",
    "test_csv = '~/Documents/Kaggle/GoogleLandmarkRecognition/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to load the train and test data\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    image_files = np.array(data['filenames'])\n",
    "    train_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return image_files, train_targets\n",
    "\n",
    "# point this to the train.csv file\n",
    "def load_variable_names(path):\n",
    "    # use dtype=None because we have strings and ints\n",
    "    data = pd.read_csv(path, quotechar='\"')\n",
    "    return data\n",
    "\n",
    "# use this to count the images available to learn from\n",
    "def get_total_files(path):\n",
    "    return len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 files in the train/ dir\n",
      "There are 1225029 files in the train.csv file\n",
      "There are 14951 unique landmarks in the train.csv file\n",
      "\n",
      "There are 117697 files in the test/ dir\n",
      "There are 117703 files in the test.csv file\n",
      "\n",
      "Missing 1225029 training files!\n",
      "Missing 6 testing files!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_files, train_targets = load_dataset('/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition')\n",
    "train_csv_pd = load_variable_names(train_csv)\n",
    "test_csv_pd = load_variable_names(test_csv)\n",
    "\n",
    "total_files_train = get_total_files(raid_train_dir)\n",
    "total_files_test = get_total_files(raid_test_dir)\n",
    "\n",
    "print('There are %d files in the train/ dir' % total_files_train)\n",
    "print('There are %d files in the train.csv file' % len(train_csv_pd['id']))\n",
    "print('There are %d unique landmarks in the train.csv file' % len(train_csv_pd['landmark_id'].unique()))\n",
    "print('\\nThere are %d files in the test/ dir' % total_files_test)\n",
    "print('There are %d files in the test.csv file' % len(test_csv_pd['id']))\n",
    "\n",
    "print('\\nMissing %d training files!' % (len(train_csv_pd['id']) - total_files_train))\n",
    "print('Missing %d testing files!' % (len(test_csv_pd['id']) - total_files_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT : Wound up with a 77-23 split because of missing files. The first time I ran it I got a 84-16 split.\n",
    "#          I tried running it again to get closer to 80 - 20 but because of missing files I ran up with 77-23.\n",
    "\n",
    "# Uncomment to split the training data\n",
    "#raid_practice = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/'\n",
    "#raid_practice_train = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/train/'\n",
    "#raid_practice_valid = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/valid/'\n",
    "#csv_file = '/mnt/raid0/Projects/Kaggle/GoogleLandmarkRecognition/test_valid_split.csv'\n",
    "\n",
    "#csv_file_data = load_variable_names(csv_file)\n",
    "#train, valid = train_test_split(csv_file_data, test_size=0.2)\n",
    "#train, valid = train_test_split(train_csv_pd, test_size=0.04)\n",
    "#print(train)\n",
    "#print('\\n')\n",
    "#print(valid)\n",
    "\n",
    "#for row in valid.itertuples():\n",
    "#    filename = str(row[3]) + '/' + str(row[1]) + '.jpg'\n",
    "#    dirname = str(row[3])\n",
    "#    if os.path.isfile(os.path.join(raid_practice_train, filename)):\n",
    "#        if not os.path.isdir(raid_practice_valid + dirname + '/'):\n",
    "#            os.makedirs(raid_practice_valid + dirname)\n",
    "#        os.rename(raid_practice_train + filename, raid_practice_valid + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to True and uncomment if you need to process the data into subdirectories\n",
    "#FORMAT_INTO_SUBDIRECTORY = False\n",
    "    \n",
    "#if FORMAT_INTO_SUBDIRECTORY:\n",
    "#    img_count = 0\n",
    "#    dir_count = 0\n",
    "#    for row in file_info_train.itertuples():\n",
    "#        filename = row[1]+'.jpg'\n",
    "#        dirname = row[3]\n",
    "#        if os.path.isfile(os.path.join(raid_train_dir,filename)):\n",
    "#            if not os.path.isdir(raid_train_dir+str(dirname)+'/'):\n",
    "#                os.makedirs(raid_train_dir+str(dirname))\n",
    "#                dir_count += 1\n",
    "#            os.rename(raid_train_dir+filename, raid_train_dir+str(dirname)+'/'+filename)\n",
    "#            img_count += 1\n",
    "#    print('Moved {0} Files into {1} new directories.'.format(img_count, dir_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 940338 images belonging to 14951 classes.\n",
      "Found 284000 images belonging to 13168 classes.\n"
     ]
    }
   ],
   "source": [
    "# Need to test and configure this code!!\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        raid_train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=1000,\n",
    "        class_mode='categorical')\n",
    "\n",
    "valid_generator = test_datagen.flow_from_directory(\n",
    "        raid_valid_dir,\n",
    "        target_size=(150,150),\n",
    "        batch_size=1000,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    model.add(Dense(14951, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(14951, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "resnet = ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "bottleneck_features_train = resnet.predict_generator(train_generator)\n",
    "# save the output as a Numpy array\n",
    "np.save(open('bottleneck_features/bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_validation = resnet.predict_generator(valid_generator)\n",
    "np.save(open('bottleneck_features/bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n",
    "\n",
    "#model = create_model()\n",
    "#model.summary()\n",
    "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#checkpointer = ModelCheckpoint(filepath=raid_dir + 'saved_models/weights.best.from_scratch.hdf5', \n",
    "#                           verbose=1, save_best_only=True)\n",
    "\n",
    "#model.fit_generator(\n",
    "#    train_generator,\n",
    "#    steps_per_epoch=940,\n",
    "#    epochs=25,\n",
    "#    validation_data=valid_generator,\n",
    "#    validation_steps=284,\n",
    "#    use_multiprocessing=True,\n",
    "#    callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
